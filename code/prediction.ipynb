{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:21.602478Z",
     "start_time": "2023-06-02T15:24:18.332176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load the BertTokenizer\n"
     ]
    }
   ],
   "source": [
    "from model import create_VST_model\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import cv2\n",
    "from keras.mixed_precision import policy\n",
    "from keras.utils import tf_inspect\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Use for texture data preprocessing\n",
    "pattern = \"[A-Z]\"\n",
    "pattern1 = '[\"\\\\[\\\\]\\\\\\\\]'\n",
    "pattern2 = \"[*.+!$#&,;{}()':=/<>%-]\"\n",
    "pattern3 = '[_]'\n",
    "\n",
    "# Define basic parameters\n",
    "max_len = 100\n",
    "training_samples = 147\n",
    "validation_samples = 63\n",
    "max_words = 1000\n",
    "\n",
    "# store all data\n",
    "data_set = {}\n",
    "\n",
    "# store file name\n",
    "file_name = []\n",
    "\n",
    "# store structure information\n",
    "data_structure = {}\n",
    "\n",
    "# store texture information\n",
    "data_texture = {}\n",
    "\n",
    "# store token, position and segment information\n",
    "data_token = {}\n",
    "data_position = {}\n",
    "data_segment = {}\n",
    "# dic_content = {}\n",
    "\n",
    "# store the content of each text\n",
    "string_content = {}\n",
    "\n",
    "# store picture information\n",
    "data_picture = {}\n",
    "\n",
    "# store content of each picture\n",
    "data_image = []\n",
    "\n",
    "# experimental part â€” randomly shuffling data\n",
    "all_data = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "structure = []\n",
    "image = []\n",
    "token = []\n",
    "segment = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:21.606293Z",
     "start_time": "2023-06-02T15:24:21.602687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load the BertTokenizer\n"
     ]
    }
   ],
   "source": [
    "# Define the basic bert class\n",
    "class BertConfig(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = kwargs.pop('vocab_size', 30000)\n",
    "        self.type_vocab_size = kwargs.pop('type_vocab_size', 300)\n",
    "        self.hidden_size = kwargs.pop('hidden_size', 768)\n",
    "        self.num_hidden_layers = kwargs.pop('num_hidden_layers', 12)\n",
    "        self.num_attention_heads = kwargs.pop('num_attention_heads', 12)\n",
    "        self.intermediate_size = kwargs.pop('intermediate_size', 3072)\n",
    "        self.hidden_activation = kwargs.pop('hidden_activation', 'gelu')\n",
    "        self.hidden_dropout_rate = kwargs.pop('hidden_dropout_rate', 0.1)\n",
    "        self.attention_dropout_rate = kwargs.pop('attention_dropout_rate', 0.1)\n",
    "        self.max_position_embeddings = kwargs.pop('max_position_embeddings', 200)\n",
    "        self.max_sequence_length = kwargs.pop('max_sequence_length', 200)\n",
    "\n",
    "\n",
    "class BertEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(name='BertEmbedding')\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.token_embedding = self.add_weight('weight', shape=[self.vocab_size, self.hidden_size],\n",
    "                                               initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
    "        self.type_vocab_size = config.type_vocab_size\n",
    "\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            name='position_embedding'\n",
    "        )\n",
    "        self.token_type_embedding = tf.keras.layers.Embedding(\n",
    "            config.type_vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            name='token_type_embedding'\n",
    "        )\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name='LayerNorm')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        with tf.name_scope('bert_embeddings'):\n",
    "            super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False, mode='embedding'):\n",
    "        # used for masked lm\n",
    "        if mode == 'linear':\n",
    "            return tf.matmul(inputs, self.token_embedding, transpose_b=True)\n",
    "\n",
    "        input_ids, token_type_ids = inputs\n",
    "        input_ids = tf.cast(input_ids, dtype=tf.int32)\n",
    "        position_ids = tf.range(input_ids.shape[1], dtype=tf.int32)[tf.newaxis, :]\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_ids.shape.as_list(), 0)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        token_type_embeddings = self.token_type_embedding(token_type_ids)\n",
    "        token_embeddings = tf.gather(self.token_embedding, input_ids)\n",
    "\n",
    "        embeddings = token_embeddings + token_type_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "        return embeddings\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the config of the layer.\n",
    "\n",
    "        A layer config is a Python dictionary (serializable)\n",
    "        containing the configuration of a layer.\n",
    "        The same layer can be reinstantiated later\n",
    "        (without its trained weights) from this configuration.\n",
    "\n",
    "        The config of a layer does not include connectivity\n",
    "        information, nor the layer class name. These are handled\n",
    "        by `Network` (one layer of abstraction above).\n",
    "\n",
    "        Returns:\n",
    "            Python dictionary.\n",
    "        \"\"\"\n",
    "        all_args = tf_inspect.getfullargspec(self.__init__).args\n",
    "        config = {\n",
    "            'name': self.name,\n",
    "            'trainable': self.trainable,\n",
    "        }\n",
    "        if hasattr(self, '_batch_input_shape'):\n",
    "            config['batch_input_shape'] = self._batch_input_shape\n",
    "        config['dtype'] = policy.serialize(self._dtype_policy)\n",
    "        if hasattr(self, 'dynamic'):\n",
    "            # Only include `dynamic` in the `config` if it is `True`\n",
    "            if self.dynamic:\n",
    "                config['dynamic'] = self.dynamic\n",
    "            elif 'dynamic' in all_args:\n",
    "                all_args.remove('dynamic')\n",
    "        expected_args = config.keys()\n",
    "        # Finds all arguments in the `__init__` that are not in the config:\n",
    "        extra_args = [arg for arg in all_args if arg not in expected_args]\n",
    "        # Check that either the only argument in the `__init__` is  `self`,\n",
    "        # or that `get_config` has been overridden:\n",
    "        if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):\n",
    "            raise NotImplementedError('Layer %s has arguments in `__init__` and '\n",
    "                                      'therefore must override `get_config`.' %\n",
    "                                      self.__class__.__name__)\n",
    "        return config\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-768_A-12')\n",
    "print('Successfully load the BertTokenizer')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:21.999551Z",
     "start_time": "2023-06-02T15:24:21.616144Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agnia.sergeyuk/Library/Caches/pypoetry/virtualenvs/readability-features-3of4OTbJ-py3.9/lib/python3.9/site-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_VST_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.537192Z",
     "start_time": "2023-06-02T15:24:22.007069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.load_weights('../Experimental output/VST_BEST.hdf5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.587771Z",
     "start_time": "2023-06-02T15:24:22.539890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "structure_dir = '../OUR_Dataset/Processed Dataset/Structure'\n",
    "texture_dir = '../OUR_Dataset/Processed Dataset/Texture'\n",
    "picture_dir = '../OUR_Dataset/Processed Dataset/Image'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.590089Z",
     "start_time": "2023-06-02T15:24:22.588138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def preprocess_new_structure_data(structure_dir):\n",
    "    for f_name in os.listdir(structure_dir):\n",
    "        f = open(os.path.join(structure_dir, f_name), errors='ignore')\n",
    "        lines = []\n",
    "        \n",
    "        if not f_name.startswith('.'):\n",
    "            file_name.append(f_name.split('.')[0])\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip(' \\n')\n",
    "                info = line.split(' ')\n",
    "                info_int = []\n",
    "                \n",
    "                count = 0\n",
    "                max_elements = 305\n",
    "                \n",
    "                for item in info:\n",
    "                    if count < max_elements:\n",
    "                        info_int.append(int(item))\n",
    "                        count += 1  \n",
    "\n",
    "                info_int = np.asarray(info_int)\n",
    "                lines.append(info_int)\n",
    "            f.close()\n",
    "            \n",
    "            lines = np.asarray(lines)\n",
    "            data_structure[f_name.split('.')[0]] = lines\n",
    "    return data_structure"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.594062Z",
     "start_time": "2023-06-02T15:24:22.592281Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def preprocess_new_texture_data(texture_dir):\n",
    "    for f_name in os.listdir(texture_dir):\n",
    "        if f_name[-4:] == \".txt\":\n",
    "            list_content = []\n",
    "            list_position = []\n",
    "            list_segment = []\n",
    "            s = ''\n",
    "            segment_id = 0\n",
    "            position_id = 0\n",
    "            count = 0\n",
    "            f = open(os.path.join(texture_dir, f_name), errors='ignore')\n",
    "            for content in f:\n",
    "                content = re.sub(r\"([a-z]+)([A-Z]+)\", r\"\\1 \\2\", content)\n",
    "                content = re.sub(pattern1, lambda x: \" \" + x.group(0) + \" \", content)\n",
    "                content = re.sub(pattern2, lambda x: \" \" + x.group(0) + \" \", content)\n",
    "                content = re.sub(pattern3, lambda x: \" \", content)\n",
    "                list_value = content.split()\n",
    "                for item in list_value:\n",
    "                    if len(item) > 1 or not item.isalpha():\n",
    "                        s = s + ' ' + item\n",
    "                        list_content.append(item)\n",
    "                        if count < max_len:\n",
    "                            list_position.append(position_id)\n",
    "                            position_id += 1\n",
    "                            list_segment.append(segment_id)\n",
    "                        count += 1\n",
    "                segment_id += 1\n",
    "            while count < max_len:\n",
    "                list_segment.append(segment_id)\n",
    "                list_position.append(count)\n",
    "                count += 1\n",
    "            f.close()\n",
    "            string_content[f_name.split('.')[0]] = s\n",
    "            data_position[f_name.split('.')[0]] = list_position\n",
    "            data_segment[f_name.split('.')[0]] = list_segment\n",
    "\n",
    "    for sample in string_content:\n",
    "        list_token = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(string_content[sample]))\n",
    "        list_token = list_token[:max_len]\n",
    "        while len(list_token) < max_len:\n",
    "            list_token.append(0)\n",
    "        data_token[sample] = list_token\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.600146Z",
     "start_time": "2023-06-02T15:24:22.598593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def preprocess_new_picture_data(picture_dir):\n",
    "    for f_name in os.listdir(picture_dir):\n",
    "        if not f_name.startswith('.') and f_name[-4:] in ['.jpg', '.jpeg', '.png']:\n",
    "            img_data = cv2.imread(os.path.join(picture_dir, f_name))\n",
    "            img_data = cv2.resize(img_data, (128, 128))\n",
    "            result = img_data / 255.0\n",
    "            data_picture[f_name.split('.')[0]] = result\n",
    "            data_image.append(result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.603477Z",
     "start_time": "2023-06-02T15:24:22.601631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def prepare_data_for_prediction():\n",
    "    count_id = 0\n",
    "    while count_id < 69 and count_id < len(file_name):\n",
    "        all_data.append(file_name[count_id])\n",
    "        count_id += 1\n",
    "    for item in all_data:\n",
    "        structure.append(data_structure[item])\n",
    "        image.append(data_picture[item])\n",
    "        token.append(data_token[item])\n",
    "        segment.append(data_segment[item])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.606391Z",
     "start_time": "2023-06-02T15:24:22.604810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "preprocess_new_structure_data(structure_dir)\n",
    "preprocess_new_texture_data(texture_dir)\n",
    "preprocess_new_picture_data(picture_dir)\n",
    "prepare_data_for_prediction()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.980595Z",
     "start_time": "2023-06-02T15:24:22.606914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# format the data\n",
    "structure = np.asarray(structure,  dtype=np.float32)\n",
    "image = np.asarray(image,  dtype=np.float32)\n",
    "token = np.asarray(token,  dtype=np.float32)\n",
    "segment = np.asarray(segment,  dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:22.985948Z",
     "start_time": "2023-06-02T15:24:22.980979Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 17:24:23.020290: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "data_to_predict = [structure, token, segment, image]\n",
    "result = model.predict(data_to_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:23.578209Z",
     "start_time": "2023-06-02T15:24:22.991619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "file_names_column = []\n",
    "predictions_column = []\n",
    "\n",
    "for i, pred in enumerate(result):\n",
    "    file_names_column.append(all_data[i])\n",
    "    predictions_column.append(pred)\n",
    "\n",
    "data = {\n",
    "    'file_Name': file_names_column,\n",
    "    'readability_score': predictions_column\n",
    "}\n",
    "\n",
    "prediction_df = pd.DataFrame(data)\n",
    "prediction_df['readability_score'] = prediction_df['readability_score'].astype(float)\n",
    "prediction_df['readability'] = round(prediction_df['readability_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:23.582406Z",
     "start_time": "2023-06-02T15:24:23.580695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   file_Name  readability_score  readability\n0         27           0.115989          0.0\n1          1           0.017252          0.0\n2         15           0.993372          1.0\n3          8           0.179276          0.0\n4         32           0.000000          0.0\n5         42           0.206883          0.0\n6         19           0.999957          1.0\n7         47           0.907458          1.0\n8         37           0.069610          0.0\n9          4           0.765248          1.0\n10        22           0.375573          0.0\n11        10           0.076837          0.0\n12        13           0.030625          0.0\n13        21           0.999997          1.0\n14         7           0.653481          1.0\n15        44           0.999954          1.0\n16        34           0.481369          0.0\n17        28           0.968138          1.0\n18        31           0.961330          1.0\n19        41           0.113254          0.0\n20        16           0.000000          0.0\n21        48           0.061206          0.0\n22        38           0.581605          1.0\n23         2           0.917569          1.0\n24        24           0.796956          1.0\n25        11           0.976203          1.0\n26        23           0.986323          1.0\n27         5           0.705068          1.0\n28        18           0.709424          1.0\n29        36           0.985860          1.0\n30        46           0.176710          0.0\n31        43           0.999470          1.0\n32        33           0.473887          0.0\n33         9           0.695759          1.0\n34        14           0.954044          1.0\n35        26           0.029088          0.0\n36        25           0.996616          1.0\n37         3           0.004931          0.0\n38        17           0.237692          0.0\n39        39           0.983417          1.0\n40        49           0.977819          1.0\n41        40           0.859737          1.0\n42        30           0.881525          1.0\n43        29           0.987793          1.0\n44        35           0.945147          1.0\n45        45           0.104824          0.0\n46         6           0.451978          0.0\n47        20           0.027087          0.0\n48        12           0.903306          1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_Name</th>\n      <th>readability_score</th>\n      <th>readability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27</td>\n      <td>0.115989</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.017252</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>0.993372</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>0.179276</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>42</td>\n      <td>0.206883</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>19</td>\n      <td>0.999957</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>47</td>\n      <td>0.907458</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>37</td>\n      <td>0.069610</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n      <td>0.765248</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>22</td>\n      <td>0.375573</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>0.076837</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0.030625</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>21</td>\n      <td>0.999997</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7</td>\n      <td>0.653481</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>44</td>\n      <td>0.999954</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>34</td>\n      <td>0.481369</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>28</td>\n      <td>0.968138</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>31</td>\n      <td>0.961330</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>41</td>\n      <td>0.113254</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>16</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>48</td>\n      <td>0.061206</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>38</td>\n      <td>0.581605</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>0.917569</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0.796956</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>11</td>\n      <td>0.976203</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>23</td>\n      <td>0.986323</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>5</td>\n      <td>0.705068</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>18</td>\n      <td>0.709424</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>36</td>\n      <td>0.985860</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>46</td>\n      <td>0.176710</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>43</td>\n      <td>0.999470</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>0.473887</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>9</td>\n      <td>0.695759</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>14</td>\n      <td>0.954044</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>26</td>\n      <td>0.029088</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>25</td>\n      <td>0.996616</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>3</td>\n      <td>0.004931</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>17</td>\n      <td>0.237692</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>0.983417</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>49</td>\n      <td>0.977819</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>40</td>\n      <td>0.859737</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>30</td>\n      <td>0.881525</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>29</td>\n      <td>0.987793</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>35</td>\n      <td>0.945147</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>0.104824</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>6</td>\n      <td>0.451978</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>20</td>\n      <td>0.027087</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>12</td>\n      <td>0.903306</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:23.592774Z",
     "start_time": "2023-06-02T15:24:23.584206Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:23.595954Z",
     "start_time": "2023-06-02T15:24:23.591763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T15:24:23.596138Z",
     "start_time": "2023-06-02T15:24:23.593812Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
